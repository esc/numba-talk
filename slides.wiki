== Introduction ==

==== @whoami@ ====

* Senior Software Engineer at Anaconda
* Working on Numba full-time
* Doing this for almost 7 years
* Actively involved in:
** Typed containers
** Release and community management
** Structured Control Flow Graphs

==== Outline ====

\tableofcontents[currentsection]

==== Numba ====

<[center]
    <<<images/numba-logo.pdf, scale=0.20>>>
[center]>

==== Numba in a Nutshell ====

* A compiler that might make your code faster
* Requires importing a decorator: @\@jit@
* And decorating functions with it
* Numba = NumPy + Mamba (fast snake)

==== Numba Explained ====

* Numba is a
** just-in-time
** type-specializing
** function compiler
** for accelerating numerically-focused Python

====  LLVM ====

* LLVM is a compiler toolkit
* Numba uses it as a compiler backend
* Access via @llvmlite@

>>>sieve_short.wiki<<<

==== Example ====

\pyfile{code/sieve.py}

==== Example ====[containsverbatim]

\begin{ipythonconsolecode}
In [2]: %timeit sieve.primes.py_func()
11.8 ms ± 109 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [3]: %timeit sieve.primes()
177 µs ± 11.2 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
\end{ipythonconsolecode}

==== Open Source Status ====

* Several companies and many open source contributors
** several FTE funded to contribute to Numba
** several non-core contributors per release
* Issue and PR lists growing
* Very active community on GitHub, Discourse

==== Community Usage ====

* Download rate via PyPi is \textasciitilde21 Hz (audible by goldfish)
* GitHub: 163126 Repositories and 6513 Packages
* Several high-profile libraries use it:
** PyData Sparse --> sparse matrix implementation
** UMAP --> Uniform Manifold Approximation
** Tardis --> Super Nova Simulator
** Pandas --> Dataframe Library
** Open AI Whisper --> Speech-to-Text
** NVIDIA RAPIDS --> GPU Library
** many, many more, we eventually stopped keeping track

== Going Deeper ==

==== Outline ====

\tableofcontents[currentsection]

==== Numba Flow ====

<[center]
    <<<images/numba_flowchart.png, scale=0.40>>>
[center]>

==== Internals ====

* Translate Python objects of supported types into representations with no CPython dependencies ("unboxing")
* Compile Python bytecode from your decorated function into machine code.
* Swap calls to builtins and NumPy functions for implementations provided by Numba (or 3rd party Numba extensions)
* Allow LLVM to inline functions, autovectorize loops, and do other optimizations you would expect from a C compiler
* Allow LLVM to exploit the instruction sets of your hardware (SSE, AVX)
* When calling the function, release the GIL if requested
* Convert return values back to Python objects ("boxing")

==== What Numba does not do ====

* Automated translation of CPython or NumPy implementations
* Automatic compilation of 3rd party libraries
* Partial compilation
* Automatic conversion of arbitrary Python types
* Change the layout of data allocated in the interpreter
* Translate entire programs
* Magically make individual NumPy functions faster

==== When is Numba unlikely to help? ====

* Whole program compilation
* Critical functions have already been converted to C or optimized Cython
* Need to interface directly with C++
* Algorithms that are not primarily numerical, e.g. string manipulation
* Exception: Numba can do pretty well at bit manipulation

== @llvmlite@ ==

==== Outline ====

\tableofcontents[currentsection]

==== @llvmlite@ ====

* Numba's LLVM binding package
* A "lightweight" Python wrapper
* Not exactly intended as a standalone package, but a few people use it anyway
* Separation of concerns, easier to develop this way
* Uses the C++ API via C -- easier to bind to from Python
* Stability is key, we only upgrade LLVM if necessary, don't chase the bleeding edge

==== @llvmlite@ releases ====

* Released in lock-step with Numba
* About two to four times a year
* Each release only supports one (sometimes two) LLVM versions
** --> several disgruntled users
* Currently we support 14 but transitioning to 15
* ZERO-based versioning
* Patch releases only to fix severe regressions
* No backports

==== @llvmlite@ packages ====

* Numba team creates two types of packages, wheels and conda packages
* The Numba team statically distributes LLVM
* Python wheels for distribution via PyPI
** @20 * 30-40 MB@
* Conda packages for distribution via the @numba@ channel on anaconda.org
** @25 * 20-40 MB@
* Other distributors choose to link dynamically
* (This often creates issues for distributors, since they also only want to support one LLVM version, usually not the one we support but the latest)
* Hot take: LLVM is not a shared library -- but something to be vendored

==== Why not @llvmpy@ ====

* llvmlite comes from an era of LLVM 3.something
* Stability was more important, Numba is the main consumer
* Only the JIT part of LLVM needed (MCJIT)
* For example: our builder-API is entirely in Python and string based
* Better to control the bindings ourselves

==== Builder-API Example ====[containsverbatim]

\pyfile{code/builder.py}

==== Builder-API Example ====[containsverbatim]

\begin{pycode}
; ModuleID = "examples/ir_fpadd.py"
target triple = "unknown-unknown-unknown"
target datalayout = ""

define double @"fpadd"(double %".1", double %".2")
{
entry:
  %"res" = fadd double %".1", %".2"
  ret double %"res"
}
\end{pycode}

==== Code Generation Example ====[containsverbatim]

<[nowiki]
\begin{pycode}
from __future__ import print_function

from ctypes import CFUNCTYPE, c_double

import llvmlite.binding as llvm


# All these initializations are required for code generation!
llvm.initialize()
llvm.initialize_native_target()
llvm.initialize_native_asmprinter()  # yes, even this one
\end{pycode}
[nowiki]>

==== Code Generation Example ====[containsverbatim]

<[nowiki]
\begin{pycode}
llvm_ir = """
   ; ModuleID = "examples/ir_fpadd.py"
   target triple = "unknown-unknown-unknown"
   target datalayout = ""

   define double @"fpadd"(double %".1", double %".2")
   {
   entry:
     %"res" = fadd double %".1", %".2"
     ret double %"res"
   }
   """
\end{pycode}
[nowiki]>

==== Code Generation Example ====[containsverbatim]

<[nowiki]
\begin{pycode}
def create_execution_engine():
    """
    Create an ExecutionEngine suitable for JIT code generation on
    the host CPU.  The engine is reusable for an arbitrary number of
    modules.
    """
    # Create a target machine representing the host
    target = llvm.Target.from_default_triple()
    target_machine = target.create_target_machine()
    # And an execution engine with an empty backing module
    backing_mod = llvm.parse_assembly("")
    engine = llvm.create_mcjit_compiler(backing_mod, target_machine)
    return engine
\end{pycode}
[nowiki]>

==== Code Generation Example ====[containsverbatim]

<[nowiki]
\begin{pycode}
def compile_ir(engine, llvm_ir):
    """
    Compile the LLVM IR string with the given engine.
    The compiled module object is returned.
    """
    # Create a LLVM module object from the IR
    mod = llvm.parse_assembly(llvm_ir)
    mod.verify()
    # Now add the module and make sure it is ready for execution
    engine.add_module(mod)
    engine.finalize_object()
    engine.run_static_constructors()
    return mod
\end{pycode}
[nowiki]>

==== Code Generation Example ====[containsverbatim]

<[nowiki]
\begin{pycode}
engine = create_execution_engine()
mod = compile_ir(engine, llvm_ir)

# Look up the function pointer (a Python int)
func_ptr = engine.get_function_address("fpadd")

# Run the function via ctypes
cfunc = CFUNCTYPE(c_double, c_double, c_double)(func_ptr)
res = cfunc(1.0, 3.5)
print("fpadd(...) =", res)
\end{pycode}
[nowiki]>

== NumPy Support ==

==== Outline ====

\tableofcontents[currentsection]

==== Implementing NumPy ====

* Numba does not use much of the NumPy C implementations
* (Only some ufuncs are "borrowed".)
* We implement the NumPy API using Numba compatible/supported Python
* Treat NumPy as DSL for array oriented computing

==== Implementing Numpy ====

* Implement: @numpy.linalg.norm@
* For vectors, @ord@ is:
** @inf@ --> @min(abs(x))@
** @0  @ --> @sum(x != 0)@
* Implemented in: @numba.targets.linalg.py@ 

==== Implementing Numpy ====[containsverbatim]

\begin{pycode}
    elif ord == -np.inf:
        # min(abs(a))
        ret = abs(a[0])
        for k in range(1, n):
            val = abs(a[k])
            if val < ret:
                ret = val
        return ret

    elif ord == 0:
        # sum(a != 0)
        ret = 0.0
        for k in range(n):
            if a[k] != 0.:
                ret += 1.
        return ret
\end{pycode}

== Tips and Tricks ==

==== Outline ====

\tableofcontents[currentsection]

==== Tips and Tricks ====

* Always use @\@jit@
* Prefer NumPy arrays for numerical data
* Use typed containers from @numba.typed@ for nested data
* @for@ loops are fine
* Array expressions are fused if on the same line, so those are fine too

==== Typed Containers ====[containsverbatim]

\pyfile{code/typed.py}

==== Typed Containers ====[containsverbatim]

\begin{consolecode}
$ python code/typed.py
[[0, 1, 2, 3, 4], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]
[[0, 1, 2, 3, 4], [23, 23, 23, 23, 23, 23, 23, 23, 23, 23]]
\end{consolecode}

==== Fused Expressions ====[containsverbatim]

\pyfile{code/fused.py}

==== Fused Expressions ====[containsverbatim]

\begin{ipythonconsolecode}
In [1]: from fused import a,b,func

In [2]: %timeit func.py_func(a,b)
4.68 ms ± 89.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [3]: %timeit func(a,b)
626 µs ± 22.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
\end{ipythonconsolecode}

==== OS and Hardware Support ====

* Windows 7 and later 64-bit only
* macOS 10.9 and later, @x86\_64@ (Intel) and @arm64@ (M1, M2 etc...)
* Linux @x86\_64@ and @aarch64@ (no @ppc64le@ anymore)
* NVIDIA CUDA GPUs (Compute capability 5.3 and later, CUDA 11.2 and later)
** --> currently being refactored into separate package @numba-cuda@

==== Python versions ====

* Python 3.10 - 3.13 as of Numba 0.62 and llvmlite 0.43

==== Packaging ====

* You can depend on Numba to perform the heavy lifting!
* We run CI on most Python/NumPy/OS/Hardware combinations
* You can ship a single source package
** PyPi
** anaconda.org
* No need to pre-compile binaries for your users

== Compiler Toolkit ==

==== Outline ====

\tableofcontents[currentsection]

==== @inspect@ methods ====[containsverbatim]

* There are various @inspect@ methods on compiled functions
** @inspect\_types@  --> Prints out results from type-inference
** @inspect\_llvm@   --> Obtain LLVM IR
** @inspect\_asm@    --> Obtain print out of function assembly
** @inspect\_cfg@    --> Obtain Control Flow Graph
** @inspect\_dissam\_cfg@  --> Control Flow Graph and disassembly from reversing generated ELF

==== Custom Compiler Passes ====

\pyfile{code/new_pass.py}

==== Custom Compiler Passes ====

\pyfile{code/new_pass_code.py}

==== Custom Compiler Passes ====

\pyfile{code/pipeline.py}

==== Custom Compiler Passes ====

\pyfile{code/use_pipeline.py}

== Summary ==

==== Outline ====

\tableofcontents[currentsection]

==== Understood Numba? ====

* Numba is a
** just-in-time
** type-specializing
** function compiler
** for accelerating numerically-focused Python

==== What is Cooking? ====

* AST/Source based frontend
* RVSDG based lambda-calculus intermediary representation
* Hindley-Milner style type inference
* egglog/Equality saturation
* PIXIE based backend, toolchain and executable format
* Transition from MCJIT --> ORCJIT
* Compiler modularization: `numba-cuda` and `numba-numpy`
* Refactoring to support NumPy 2.0 type system

--0.5cm--

* Effectively: complete project overhaul

==== Getting in Touch ====

* https://numba.pydata.org
* https://github.com/numba
* https://numba.discourse.group/
* Preferred: GitHub + Gitter + Discourse

* Stuck? It can't hurt to ask. ;-)
